{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f00def",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_Step2_NAME=\"Sahamyab-Session_16_2\"\n",
    "KAFKA_SERVER=\"kafka-broker:29092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0745a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']='--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"Count-Hashtags\") \\\n",
    "    .config(\"spark.executor.memory\", \"1024mb\") \\\n",
    "    .config(\"spark.executor.cores\",\"1\") \\\n",
    "    .config(\"spark.cores.max\", \"1\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Tehran\") \\\n",
    "    .getOrCreate()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f05896",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"id\", StringType(), True),\\\n",
    "                         StructField(\"content\", StringType(), True),\\\n",
    "                         StructField(\"sendTime\", StringType(), True),\\\n",
    "                         StructField(\"sendTimePersian\", StringType(), True),\\\n",
    "                         StructField(\"senderName\", StringType(), True),\\\n",
    "                         StructField(\"senderUsername\", StringType(), True),\\\n",
    "                         StructField(\"type\", StringType(), True),\\\n",
    "                         StructField(\"hashtags\", ArrayType(StringType()), True)\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f193720",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "  .option(\"subscribe\", TOPIC_Step2_NAME) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"kafka.group.id\", \"Count-Hashtags-and-write to-Stream\")\\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsStringDF = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e41ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF = tweetsStringDF.select(from_json(col(\"value\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "tweetsDF = tweetsDF.withColumn(\"timestamp\", unix_timestamp(\"sendTime\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\").cast('timestamp')) \\\n",
    "            .withColumn(\"persian_timestamp\", from_utc_timestamp(\"timestamp\", \"Asia/Tehran\").cast('timestamp')) \\\n",
    "            .withColumn(\"persianYear\", tweetsDF['sendTimePersian'].substr(0, 4)) \\\n",
    "            .withColumn(\"persianMonth\", tweetsDF['sendTimePersian'].substr(6, 2)) \\\n",
    "            .withColumn(\"persianDay\", tweetsDF['sendTimePersian'].substr(9, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagCounts = tweetsDF.select(explode(\"hashtags\").alias(\"hashtag\")) \\\n",
    "                      .groupBy(\"hashtag\")\\\n",
    "                      .count()\\\n",
    "                      .orderBy(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd55a20",
   "metadata": {},
   "source": [
    "This code (above cell) transforms the `tweetsDF` DataFrame to compute the counts of hashtags. It creates a new DataFrame `hashtagCounts` that contains two columns: \"hashtag\" and \"count\". The \"hashtag\" column contains the unique hashtags, and the \"count\" column contains the number of occurrences of each hashtag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = hashtagCounts.writeStream\\\n",
    "                  .outputMode(\"complete\")\\\n",
    "                  .format(\"console\")\\\n",
    "                  .option(\"truncate\", \"false\")\\\n",
    "                  .option(\"numRows\",\"20\")\\\n",
    "                  .start()\\\n",
    "                  .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabb126",
   "metadata": {},
   "source": [
    "This code (above cell) writes the contents of the `hashtagCounts` DataFrame to the console as a stream. \n",
    "\n",
    "`.outputMode(\"complete\")`: This sets the output mode for the streaming query. In this case, it is set to \"complete\", which means that all rows in the result table will be written to the console every time there is an update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9679cc66",
   "metadata": {},
   "source": [
    "### Submit Sample Spark App in Pyspark Container Bash "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66558c8",
   "metadata": {},
   "source": [
    "# - Go to Pyspark Shell :\n",
    "```bash\n",
    "docker exec -it pyspark bash\n",
    "```\n",
    "- cd /opt/spark-apps/\n",
    "\n",
    "```bash\n",
    "unset PYSPARK_DRIVER_PYTHON\n",
    "spark-submit --master  spark-master:7077  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 NAME_OF_YOUR_FILE.py\n",
    "export PYSPARK_DRIVER_PYTHON=python\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3013634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
