{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f00def",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_Step2_NAME=\"Sahamyab-Session_16_2\"\n",
    "KAFKA_SERVER=\"kafka-broker:29092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28595399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10_2.12\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']='--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f5b82f",
   "metadata": {},
   "source": [
    "## Note: \n",
    "`org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1` is a package that provides integration between Apache Spark’s Structured Streaming and Apache Kafka. This package allows you to read data from and write data to Kafka using Spark’s Structured Streaming API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"Hashtag-Processing\") \\\n",
    "    .config(\"spark.executor.memory\", \"512mb\") \\\n",
    "    .config(\"spark.executor.cores\",\"1\") \\\n",
    "    .config(\"spark.cores.max\", \"1\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Tehran\") \\\n",
    "    .getOrCreate()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d219c6",
   "metadata": {},
   "source": [
    " `.config(\"spark.cores.max\", \"1\")`: This sets the maximum amount of CPU cores to request for the application from across the cluster (not necessarily from a single machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f05896",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"id\", StringType(), True),\\\n",
    "                         StructField(\"content\", StringType(), True),\\\n",
    "                         StructField(\"sendTime\", StringType(), True),\\\n",
    "                         StructField(\"sendTimePersian\", StringType(), True),\\\n",
    "                         StructField(\"senderName\", StringType(), True),\\\n",
    "                         StructField(\"senderUsername\", StringType(), True),\\\n",
    "                         StructField(\"type\", StringType(), True),\\\n",
    "                         StructField(\"hashtags\", ArrayType(StringType()), True)\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f193720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a DataFrame that reads data from a Kafka topic using Spark’s Structured Streaming API.\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "  .option(\"subscribe\", TOPIC_Step2_NAME) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74b03a",
   "metadata": {},
   "source": [
    "`.option(\"subscribe\", TOPIC_Step2_NAME)`: The DataFrame will read data from the Kafka topic with  name \"TOPIC_Step2_NAME\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38459494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line of code creates a new DataFrame by selecting and transforming columns from an \n",
    "# existing DataFrame using SQL expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf95455",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsStringDF = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "tweetsDF = tweetsStringDF.select(from_json(col(\"value\"), schema).alias(\"data\")).select(\"data.*\") \n",
    "\n",
    "tweetsDF = tweetsDF.withColumn(\"timestamp\", unix_timestamp(\"sendTime\", \"yyyy-MM-dd'T'HH:mm:ssz\").cast('timestamp')) \\\n",
    "            .withColumn(\"persianYear\", tweetsDF['sendTimePersian'].substr(0, 4)) \\\n",
    "            .withColumn(\"persianMonth\", tweetsDF['sendTimePersian'].substr(6, 2)) \\\n",
    "            .withColumn(\"persianDay\", tweetsDF['sendTimePersian'].substr(9, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a732c0",
   "metadata": {},
   "source": [
    "`.select(\"data.*\")`: This selects all columns from the \"data\" column, which is a struct type. The \"*\" is a wildcard that matches all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d58aa",
   "metadata": {},
   "source": [
    "`.withColumn(\"persianYear\", tweetsDF['sendTimePersian'].substr(0, 4))`: This line adds a new column named \"persianYear\" to the DataFrame. The values for this column are computed by extracting a substring of length 4 starting from position 0 from the \"sendTimePersian\" column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9134280",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF.select(col(\"id\"), col(\"sendTime\"), col(\"senderName\"), col(\"persianYear\"),col(\"persianMonth\"), col(\"persianDay\"))\\\n",
    "      .writeStream \\\n",
    "      .format(\"console\") \\\n",
    "      .outputMode(\"append\") \\\n",
    "      .start() \\\n",
    "      .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7521da",
   "metadata": {},
   "source": [
    "This code (above cell) uses the `writeStream` method of the DataFrame class in PySpark to write the contents of the tweetsDF DataFrame to the console as a stream. \n",
    "\n",
    "`.format(\"console\")`: This specifies the format of the data sink. In this case, it is set to \"console\", which means that the data will be written to the console."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
